{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariya-kislicyna/ML_DS22/blob/main/03_homework_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "За последнее домашнее задание можно набрать 50 баллов – *таким образом, можно набрать до 5 дополнительных баллов*, которые могут помочь добрать где-либо ранее потерянные баллы, если есть такая необходимость.\n",
        "\n",
        "**Дедлайн – 24 декабря, 23:59**. *Крайне приветствуется* сдача в ранний срок – работа будет проверена практически сразу (в рабочие часы) и студенту будет выставлены итоговые баллы**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fc5aIKvyyOkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.** (5 баллов) Вы разработали модель, которая страдает низким смещением и высокой дисперсией. Как Вы думаете, какой метод/алгоритм можно в таком случае применить и почему? "
      ],
      "metadata": {
        "id": "9B4ww11_Wdw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Алгоритмы нелинейного машинного обучения часто имеют низкое смещение и высокую дисперсию.*\n",
        "> \n",
        "*Алгоритм k-ближайших соседей имеет низкое смещение и высокую дисперсию, но компромисс можно изменить, увеличив значение k, которое увеличивает количество соседей, которые вносят вклад в прогноз, и,в свою очередь, увеличивает смещение модели.*\n",
        "> \n",
        "*Алгоритм машины опорных векторов имеет низкое смещение и высокую дисперсию, но компромисс можно изменить, увеличив параметр C, который влияет на количество нарушений допустимого запаса в обучающих данных, что увеличивает смещение, но уменьшает дисперсию.*"
      ],
      "metadata": {
        "id": "xM8YvSwQpOvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.** (5 баллов) У Вас есть датафрейм `df_baguettes`, содержащий список цен на французские багеты. Но оказалось, что в этом датафрейме отсутствуют многие значения в столбце цен. По крайней мере несколько багетов имеют цену в столбце.\n",
        "Напишите функцию `median_baguettes`, которая вычисляет медианную цену выбранных французских багетов вместо отсутствующих значений. "
      ],
      "metadata": {
        "id": "ZWtaoeHFWeAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def median_baguettes (df_baguettes):\n",
        "    \n",
        "    for col in df_baguettes.select_dtypes(['object']).columns:\n",
        "        df_baguettes[col] = df_baguettes[col].fillna(df_baguettes[col].median())\n",
        "    \n",
        "    return df_baguettes"
      ],
      "metadata": {
        "id": "ST8Ta1-6hP5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.** (5 баллов) Предположим, что у Вас есть набор данных как с непрерывными, так и с категориальными переменными. Какие методы кластеризации не помогут достичь высокого качества построенной модели при описанной ситуации с данными и почему? И какие методы кластеризации Вы бы использовали? "
      ],
      "metadata": {
        "id": "o4vriMdTZrpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*При кластеризации, популярным выбором метрики для измерения расстояния между точками является евклидово расстояние. Также иногда используется манхэттенского расстояния для определенных типов задач. Однако обе эти метрики расстояния применимы только к непрерывным данным. В наших данных, которые содержат как непрерывные, так и с категориальные переменные, евклидовы и манхэттенские расстояния не получится применить, и поэтому такие алгоритмы, как K-средние и иерархическая кластеризация, не будут работать.*\n",
        ">\n",
        "*В данном случае можно использовать подход к кластеризации с использованием расстояния Гауэра, метода Partitioning Around Medoids (K-medoids) и метод силуэта (Silhouette Method).*\n",
        ">\n",
        "*Расстояния Гауэра является метрикой, которую можно использовать для расчета расстояния между двумя объектами, атрибуты которых представляют собой сочетание категориальных и количественных значений.Это расстояние масштабируется в числовом диапазоне от 0 (идентично) до 1 (максимально отличается).*\n",
        ">\n",
        "*Алгоритм Partitioning Around Medoids (PAM) работает вместе с расстоянием Гауэра. PAM — это итеративная процедура кластеризации, аналогичная K-средним, но с некоторыми небольшими отличиями. Вместо центроидов в кластеризации K-средних PAM повторяется снова и снова, пока медоиды не изменят свои позиции. Медоид кластера — это элемент кластера, представляющий медиану всех рассматриваемых атрибутов.*\n",
        ">\n",
        "*Метод силуэта — один из самых популярных вариантов выбора оптимального количества кластеров. Он измеряет сходство каждой точки с ее кластером и сравнивает это со сходством точки с ближайшим соседним кластером. Этот показатель находится в диапазоне от -1 до 1, где более высокое значение означает большее сходство точек с их кластерами.*\n",
        "\n"
      ],
      "metadata": {
        "id": "NFf3GqaUEadh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.** Предположим, что перед Вами стоит задача: построить модель машинного обучения для прогнозирования заполняемости отеля в любую дату.\n",
        "\n",
        "\n",
        "*   (2.5 баллов) Какие данные Вы бы использовали для обучения вашей модели? \n",
        "*   (10 баллов со скелетом кода, 5 баллов с письменным описанием) Какую бы модель Вы разработали? \n",
        "*   (2.5 баллов) Как бы Вы оценивали качество модели? \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SKfQdhKceSY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Важны как текущие, так и исторические данные: расположение отеля или недвижимости, количество гостиничных номеров и их размер, цены, качество обслуживания, текущий рыночный спрос, сезонность и праздники, а также экономические изменения, мероприятия (спортивные соревнования, концерты, конференции), обменные курсы, политические ситуации и ценообразование конкурентов.\n",
        "> \n",
        "Я бы подготовила данные (типичная процедура), разделила на train и test сет:\n",
        "\n",
        "\n",
        "```\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "```\n",
        "Давайте срезу перейдем к разработке модели. Первая модель, которую я бы использовала - KNN:\n",
        "\n",
        "```\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
        "conf = confusion_matrix(y_test, y_pred_knn)\n",
        "clf_report = classification_report(y_test, y_pred_knn)\n",
        "\n",
        "print(f\"Accuracy Score of KNN is : {acc_knn}\")\n",
        "print(f\"Confusion Matrix : \\n{conf}\")\n",
        "print(f\"Classification Report : \\n{clf_report}\")\n",
        "```\n",
        "Качество модели оцениваем с помощью accuracy score, f1-score (определяем с помощью clf_report)\n",
        "\n",
        "\n",
        "> \n",
        "Также можно использовать Decision Tree Classifier:\n",
        "\n",
        "\n",
        "```\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dtc = dtc.predict(X_test)\n",
        "\n",
        "acc_dtc = accuracy_score(y_test, y_pred_dtc)\n",
        "conf = confusion_matrix(y_test, y_pred_dtc)\n",
        "clf_report = classification_report(y_test, y_pred_dtc)\n",
        "\n",
        "print(f\"Accuracy Score of Decision Tree is : {acc_dtc}\")\n",
        "print(f\"Confusion Matrix : \\n{conf}\")\n",
        "print(f\"Classification Report : \\n{clf_report}\")\n",
        "```\n",
        "Сравниваем модели по accuracy\n",
        "\n",
        "\n",
        "```\n",
        "models = pd.DataFrame({\n",
        "    'Model' : [ 'KNN', 'Decision Tree Classifier'],\n",
        "    'Score' : [acc_knn, acc_dtc]\n",
        "})\n",
        "\n",
        "\n",
        "models.sort_values(by = 'Score', ascending = False)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E78xoAz7JrwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.** (10 баллов) У Вас есть строка, напоминающая объявление списка словарей. Не используя Pandas, напишите функцию `read_split_from_str`, которая:\n",
        "\n",
        "*   Читает данные и кодирует их как список словарей (данные можно сгенерировать случайно при помощи numpy, например);\n",
        "*   Разделяет данные на два списка: train и test в соотношении 70:30 и, соответственно, возвращает список `[training_set,testing_set]`. \n",
        "\n",
        "Пример входных данных:\n",
        "\n",
        "`list_of_dict_string = \"[{'x': 0, 'y': 4}, {'x': 20, 'y': 104}, {'x': 128, 'y': 212}]\"`\n",
        "\n",
        "Пример выходных данных:\n",
        "\n",
        "```\n",
        "def read_split_from_str(list_of_dict_string) \n",
        "    [\n",
        "    [{'x': 0, 'y': 4}, {'x': 20, 'y': 104}],\n",
        "    [{'x': 128, 'y': 212}]\n",
        "    ]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cm4nAmkycnfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# импортируем нужные библиотеки\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "Qa7I4YGAM42p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_dict_string = \"[{'x': 0, 'y': 4}, {'x': 20, 'y': 104}, {'x': 128, 'y': 212}]\"\n",
        "list_of_dict_string_1 = '''[\n",
        "  {'x': 0, 'y': 4}, \n",
        "  {'x': 20, 'y': 104}, \n",
        "  {'x': 128, 'y': 212}, \n",
        "  {'x': 9, 'y': 10}, \n",
        "  {'x': 7, 'y': 8}, \n",
        "  {'x': 5, 'y': 6}, \n",
        "  {'x': 3, 'y': 4}, \n",
        "  {'x': 1, 'y': 2}\n",
        "  ]'''\n",
        "def read_split_from_str(list_of_dict_string): \n",
        "    prepared_list = json.loads(list_of_dict_string.replace('\\'', '\"'))\n",
        "    up_bound = round(len(prepared_list) * 0.7)\n",
        "    train, test = [prepared_list[:round(len(prepared_list) * 0.7)]], [prepared_list[round(len(prepared_list) * 0.7):]]\n",
        "    return train + test\n",
        "read_split_from_str(list_of_dict_string_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBrr8eHhM9wQ",
        "outputId": "9ea176a5-03b9-4163-96b1-6ac2e61a11af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'x': 0, 'y': 4},\n",
              "  {'x': 20, 'y': 104},\n",
              "  {'x': 128, 'y': 212},\n",
              "  {'x': 9, 'y': 10},\n",
              "  {'x': 7, 'y': 8},\n",
              "  {'x': 5, 'y': 6}],\n",
              " [{'x': 3, 'y': 4}, {'x': 1, 'y': 2}]]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.** (10 баллов) Разработайте модель классификации k-ближайших соседей не используя scikit-learn, соблюдая некоторые условия: \n",
        "\n",
        "\n",
        "*   В качестве метрики близости использовано евклидово расстояние;\n",
        "*   Модель обратаывает датафреймы произвольного количества строк/столбцов."
      ],
      "metadata": {
        "id": "W2NbvSMrdvZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрела документацию, чтобы лучше понять, что нужно сделать\n",
        "from sklearn import neighbors\n",
        "print(neighbors.KNeighborsClassifier.__doc__)"
      ],
      "metadata": {
        "id": "h9NtTN1qNByJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris() #для разработки модели буду использовать известный iris датасет  для теста\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 42, test_size = 0.2)\n",
        "\n",
        "#сделаем выбор расстояния (по умолчанию Евклидово)\n",
        "\n",
        "def euclidean_distance(X_1, X_2):\n",
        "    dist = np.sqrt(sum((x - y) ** 2 for x, y in zip(X_1, X_2)))\n",
        "    return dist\n",
        "def manhattan_distance(X_1, X_2):\n",
        "    dist = sum(abs(x - y) for (x, y) in zip(X_1, X_2))\n",
        "    return dist\n",
        "\n",
        "#реализую модель классом\n",
        "\n",
        "class K_Nearest_Neighbor:\n",
        "    def __init__(self, n_neighbors=5, type_dist='euclidean'):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.dist = type_dist\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        \n",
        "    def predict(self, X_test):\n",
        "        pred = []\n",
        "        for i in range(len(X_test)):\n",
        "            dist_list = []\n",
        "            classes = []\n",
        "            if self.dist == 'euclidean':\n",
        "              [dist_list.append([euclidean_distance(self.X_train[j] , X_test[i]), j]) for j in  range(len(self.X_train))]\n",
        "            if self.dist == 'manhattan':\n",
        "              [dist_list.append([manhattan_distance(self.X_train[j], X_test[i]), j]) for j in  range(len(self.X_train))]\n",
        "            dist_list = sorted(dist_list)[0:self.n_neighbors]\n",
        "            for i, j in dist_list:\n",
        "                classes.append(y_train[j])\n",
        "            ans = Counter(classes).most_common(1)[0][0]\n",
        "            pred.append(ans)  \n",
        "        return pred\n",
        "    \n",
        "    def accuracy_score(self, X_test, y_test):\n",
        "        pred = self.predict(X_test)\n",
        "        cnt = 0 \n",
        "        for i, j in zip(y_test, pred):\n",
        "          if i == j:\n",
        "            cnt += 1\n",
        "        a_s = cnt / len(y_test)  \n",
        "        return a_s\n",
        "\n",
        "example_model = K_Nearest_Neighbor(2) # we can define it manually\n",
        "example_model.fit(X_train, y_train)\n",
        "prediction = example_model.predict(X_test)\n",
        "print(*prediction)\n",
        "print(f'Check our accuracy {example_model.accuracy_score(X_test, y_test)} and from sklearn {accuracy_score(y_test, prediction)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS605SmXNG8A",
        "outputId": "e37114b3-01cd-49f6-9c51-5db7351745a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0\n",
            "Check our accuracy 1.0 and from sklearn 1.0\n"
          ]
        }
      ]
    }
  ]
}
